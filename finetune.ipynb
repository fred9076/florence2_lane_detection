{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, AutoModelForCausalLM, AutoProcessor, get_scheduler\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "project_dir = os.getcwd() \n",
    "\n",
    "# Environment variables\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"Put your huggingface api key here\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' # Put your gpu id here\n",
    "\n",
    "# Constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "CHECKPOINT = \"microsoft/Florence-2-base-ft\" # your can try larger model if you have enough gpu memory\n",
    "REVISION = 'refs/pr/6'\n",
    "start_epoch = '0'\n",
    "ckpt_dir = f'{project_dir}/tulane_model_checkpoints'\n",
    "output_dir = f'{project_dir}/output'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA and florence-2 model\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "EPOCHS = 50\n",
    "LR = 4e-6\n",
    "\n",
    "# Setup LoRA\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "\n",
    "if start_epoch != '0':\n",
    "    model_id = ckpt_dir + '/epoch_' + start_epoch # Continue to train\n",
    "else:\n",
    "    model_id = CHECKPOINT # Train from scratch\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(DEVICE)\n",
    "\n",
    "if start_epoch == '0':\n",
    "    peft_model = get_peft_model(model, config)\n",
    "else:\n",
    "    peft_model = PeftModel.from_pretrained(model, model_id, is_trainable=True) # Continue to train\n",
    "\n",
    "peft_model.print_trainable_parameters()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset and dataloaders\n",
    "\n",
    "class LaneDetectionDataset(Dataset):\n",
    "    def __init__(self, jsonl_file_path: str, image_directory_path: str, test_mode: bool = False):\n",
    "        self.jsonl_file_path = jsonl_file_path\n",
    "        self.image_directory_path = image_directory_path\n",
    "        self.test_mode = test_mode\n",
    "        self.entries = self._load_entries()\n",
    "\n",
    "    def _load_entries(self) -> List[Dict[str, Any]]:\n",
    "        if self.test_mode:\n",
    "            return [{\"image\": f, \"prefix\": \"<OD_LANE>\", \"suffix\": \"\"} for f in os.listdir(self.image_directory_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        entries = []\n",
    "        with open(self.jsonl_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                entries.append(json.loads(line))\n",
    "        return entries\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:\n",
    "        entry = self.entries[idx]\n",
    "        image_path = os.path.join(self.image_directory_path, entry['image'])\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return image, None\n",
    "        \n",
    "        return image, {'prefix': entry['prefix'], 'suffix': entry['suffix']}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, data = zip(*batch)\n",
    "    if data[0] is None:  # Test mode\n",
    "        return images\n",
    "    \n",
    "    questions = [item['prefix'] for item in data]\n",
    "    answers = [item['suffix'] for item in data]\n",
    "    inputs = processor(text=questions, images=images, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    return inputs, answers\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = LaneDetectionDataset(\n",
    "    jsonl_file_path=f\"{project_dir}/florence2lane_data/tulane_florence/train/annotations.json\",\n",
    "    image_directory_path=f\"{project_dir}/florence2lane_data/tulane_florence/train/images\",\n",
    "    test_mode=False\n",
    ")\n",
    "val_dataset = LaneDetectionDataset(\n",
    "    jsonl_file_path=f\"{project_dir}/florence2lane_data/tulane_florence/valid/annotations.json\",\n",
    "    image_directory_path=f\"{project_dir}/florence2lane_data/tulane_florence/valid/images\",\n",
    "    test_mode=False\n",
    ")\n",
    "test_dataset = LaneDetectionDataset(\n",
    "    jsonl_file_path=None,\n",
    "    image_directory_path=f\"{project_dir}/florence2lane_data/tulane_florence/test/images\", # put your test images here\n",
    "    test_mode=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_lanes(lane_string, imgHgt, imgWdt):\n",
    "    \"\"\"\n",
    "    Parse the lane string into a list of polylines.\n",
    "    \"\"\"\n",
    "    parsed_lane = {'labels': [], 'lanes': []}\n",
    "    lanes = [lane for lane in lane_string.split('lane') if lane]\n",
    "    \n",
    "    for i, lane in enumerate(lanes):\n",
    "        parsed_lane['labels'].append('lane')\n",
    "        coords = re.findall(r'<loc_(\\d+)>', lane)\n",
    "        if len(coords) % 2 != 0:\n",
    "            coords = coords[:-1]\n",
    "        \n",
    "        coord_pairs = [(np.clip(float(coords[i])/1000.0, 0.0, 1.0)*(imgWdt-1), \n",
    "                        np.clip(float(coords[i+1])/1000.0, 0.0, 1.0)*(imgHgt-1)) \n",
    "                       for i in range(0, len(coords), 2)]\n",
    "        \n",
    "        line = LineString(coord_pairs)\n",
    "        parsed_lane['lanes'].append(line)\n",
    "    \n",
    "    return parsed_lane\n",
    "\n",
    "def plot_polylines_on_image(parsed_lanes, img, img_idx: int, save_img: bool = False, epoch: int = 0):\n",
    "    \"\"\"\n",
    "    Plot the polylines on the image.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    cmap = plt.cm.get_cmap('tab10')\n",
    "    \n",
    "    for i, (line, label) in enumerate(zip(parsed_lanes['lanes'], parsed_lanes['labels'])):\n",
    "        x, y = line.xy\n",
    "        ax.plot(x[0], y[0], 'o', color='red')\n",
    "        ax.plot(x[-1], y[-1], 'o', color='blue')\n",
    "        ax.plot(x, y, label=f\"lane {i}\", color=cmap(i), alpha=0.8)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)    \n",
    "\n",
    "    plt.tight_layout()  \n",
    "    \n",
    "    if save_img:\n",
    "        dir = f\"{output_dir}/inference_results/epoch_{epoch}\"\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        img_name = f\"image_{img_idx}_epoch_{epoch}.png\"\n",
    "        plt.savefig(f\"{dir}/{img_name}\", bbox_inches='tight')  \n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig) \n",
    "\n",
    "def florence2_inference_results(model, dataset: LaneDetectionDataset, count: int, epoch: int, save_img: bool = False):\n",
    "    \"\"\"\n",
    "    Inference the results of the model.\n",
    "    \"\"\"\n",
    "    count = min(count, len(dataset))\n",
    "    for i in range(count):\n",
    "        image, data = dataset[i]\n",
    "        if data is None:  # Test mode\n",
    "            prefix = \"<OD_LANE>\"\n",
    "        else:\n",
    "            prefix = data['prefix']\n",
    "        \n",
    "        imgWdt, imgHgt = image.size\n",
    "        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024,\n",
    "            early_stopping=False,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "        )\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "        parsed_answer = processor.post_process_generation(\n",
    "            generated_text,\n",
    "            task='<OD_LANE>',\n",
    "            image_size=(image.width, image.height))\n",
    "        \n",
    "        lane_string = parsed_answer['<OD_LANE>']\n",
    "        lane_result = parse_lanes(lane_string, imgHgt, imgWdt)\n",
    "        plot_polylines_on_image(lane_result, image, img_idx=i, save_img=save_img, epoch=epoch)\n",
    "        \n",
    "    \n",
    "    return lane_result\n",
    "\n",
    "def train_model(train_loader, val_loader, model, processor, epochs=EPOCHS, start_epoch= start_epoch, model_id = model_id, lr=LR):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs - start_epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {start_epoch + epoch + 1}/{epochs}\"):\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            pixel_values = inputs[\"pixel_values\"]\n",
    "            labels = processor.tokenizer(\n",
    "                text=answers,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                return_token_type_ids=False\n",
    "            ).input_ids.to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {start_epoch + epoch + 1}/{epochs}\"):\n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "                pixel_values = inputs[\"pixel_values\"]\n",
    "                labels = processor.tokenizer(\n",
    "                    text=answers,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    return_token_type_ids=False\n",
    "                ).input_ids.to(DEVICE)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        florence2_inference_results(model, val_loader.dataset, 2, start_epoch + epoch, save_img=False)\n",
    "\n",
    "        ckpt_save_dir = f\"{ckpt_dir}/epoch_{start_epoch + epoch+1}\"\n",
    "        os.makedirs(ckpt_save_dir, exist_ok=True)\n",
    "        model.save_pretrained(ckpt_save_dir)\n",
    "        processor.save_pretrained(ckpt_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test before training, if starting from scratch, the model have no idea what <OD_LANE> means\n",
    "florence2_inference_results(peft_model, val_dataset, epoch= 0, count= 2, save_img=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, start_epoch= int(start_epoch), model_id = model_id, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation after training\n",
    "florence2_inference_results(peft_model, val_dataset, epoch= EPOCHS, count= 10, save_img=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
